## Local Install

It is highly recommended to use virtual environment when installing anomalib. For instance, with [anaconda](https://www.anaconda.com/products/individual), `anomalib` could be installed as,

```bash
yes | conda create -n anomalib_env python=3.8
conda activate anomalib_env
git clone https://github.com/openvinotoolkit/anomalib.git
cd anomalib
pip install -e .
```

## Visualizer

`./anomalib/post_processing.py` 调整保存图片的大小

```python
        # figure_size = (num_cols * 3, 3)
        figure_size = (num_cols * 15, 15)   # 调整图片大小
```

`./anomalib/utils/callbacks/visualizer_callback.py` 调用的是上面的`post_processing.py`中的`Visualizer`

`VisualizerCallback`是`callbacks`，会放进`Trainer`中运行。



# Training

## ⚠️ Anomalib < v.0.4.0

By default [`python tools/train.py`](https://github.com/openvinotoolkit/anomalib/blob/main/tools/train.py)
runs [PADIM](https://arxiv.org/abs/2011.08785) model on `leather` category from the [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) [(CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/) dataset.

```bash
python tools/train.py    # Train PADIM on MVTec AD leather
```

Training a model on a specific dataset and category requires further configuration. Each model has its own configuration
file, [`config.yaml`](https://github.com/openvinotoolkit/anomalib/blob/main/configs/model/padim.yaml)
, which contains data, model and training configurable parameters. To train a specific model on a specific dataset and
category, the config file is to be provided:

```bash
python tools/train.py --config <path/to/model/config.yaml>
```

For example, to train [PADIM](anomalib/models/padim) you can use

```bash
python tools/train.py --config anomalib/models/padim/config.yaml
```

Alternatively, a model name could also be provided as an argument, where the scripts automatically finds the corresponding config file.

>  max_epochs=1

```bash
python tools/train.py --config anomalib/models/patchcore/custom_config.yaml
python tools/train.py --config anomalib/models/padim/custom_config.yaml
python tools/train.py --config anomalib/models/dfm/custom_config.yaml
python tools/train.py --config anomalib/models/dfkde/custom_config.yaml
```

>  max_epochs>1

```shell
python tools/train.py --model anomalib/models/cflow/custom_config.yaml
python tools/train.py --model anomalib/models/ganomaly/custom_config.yaml
python tools/train.py --model anomalib/models/stfpm/custom_config.yaml
```

where the currently available models are:

- [CFlow](anomalib/models/cflow)
- [DFM](anomalib/models/dfm)
- [DFKDE](anomalib/models/dfkde)
- [FastFlow](anomalib/models/fastflow)
- [PatchCore](anomalib/models/patchcore)
- [PADIM](anomalib/models/padim)
- [STFPM](anomalib/models/stfpm)
- [GANomaly](anomalib/models/ganomaly)

## Feature extraction & (pre-trained) backbones

The pre-trained backbones come from [PyTorch Image Models (timm)](https://github.com/rwightman/pytorch-image-models), which are wrapped by `FeatureExtractor`.

For more information, please check our documentation or the [section about feature extraction in "Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide"](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055#b83b:~:text=ready%20to%20train!-,Feature%20Extraction,-timm%20models%20also>).

Tips:

- Papers With Code has an interface to easily browse models available in timm: [https://paperswithcode.com/lib/timm](https://paperswithcode.com/lib/timm)

- You can also find them with the function `timm.list_models("resnet*", pretrained=True)`

The backbone can be set in the config file, two examples below.

Anomalib < v.0.4.0

```yaml
model:
  name: cflow
  backbone: wide_resnet50_2
  pre_trained: true
Anomalib > v.0.4.0 Beta - Subject to Change
```

Anomalib >= v.0.4.0

```yaml
model:
  class_path: anomalib.models.Cflow
  init_args:
    backbone: wide_resnet50_2
    pre_trained: true
```

## Custom Dataset

It is also possible to train on a custom folder dataset. To do so, `data` section in `config.yaml` is to be modified as follows:

```yaml
dataset:
  name: <name-of-the-dataset>
  format: folder
  path: <path/to/folder/dataset>
  normal_dir: normal # name of the folder containing normal images.
  abnormal_dir: abnormal # name of the folder containing abnormal images.
  normal_test_dir: null # name of the folder containing normal test images.
  task: segmentation # classification or segmentation
  mask: <path/to/mask/annotations> #optional
  extensions: null
  split_ratio: 0.2 # ratio of the normal images that will be used to create a test split
  image_size: 256
  train_batch_size: 32
  test_batch_size: 32
  num_workers: 8
  transform_config:
    train: null
    val: null
  create_validation_set: true
  tiling:
    apply: false
    tile_size: null
    stride: null
    remove_border_count: 0
    use_random_tiling: False
    random_tile_count: 16
```

> example
```yaml
dataset:
  name: <name-of-the-dataset>
  format: folder
  path: ./datasets/some
  normal_dir: 0.normal # name of the folder containing normal images.
  abnormal_dir: 1.abnormal # name of the folder containing abnormal images.
  normal_test_dir: null # name of the folder containing normal test images.
  task: segmentation # classification or segmentation
  mask: null # optional
  extensions: null
  split_ratio: 0.2  # ratio of the normal images that will be used to create a test split
  image_size: 512
  train_batch_size: 1
  test_batch_size: 1
  num_workers: 1
  transform_config:
    train: null
    val: null
  create_validation_set: true
  tiling:
    apply: false
    tile_size: null
    stride: null
    remove_border_count: 0
    use_random_tiling: False
    random_tile_count: 16
```

> 文件夹实例
```python
├── datasets
│   └── some
│       ├── 0.normal
│       │   normal images
│       └── 1.abnormal
│           abnormalimages
```

----

### ⚠️ Anomalib > v.0.4.0 Beta - Subject to Change
We introduce a new CLI approach that uses [PyTorch Lightning CLI](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html). To train a model using the new CLI, one would call the following:
```bash
anomalib fit --config <path/to/new/config/file>
```

For instance, to train a [PatchCore](https://github.com/openvinotoolkit/anomalib/tree/development/anomalib/models/patchcore) model, the following command would be run:
```bash
anomalib fit --config ./configs/model/patchcore.yaml
```

The new CLI approach offers a lot more flexibility, details of which are explained in the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html).

## Inference
### ⚠️ Anomalib < v.0.4.0

### ⚠️ Anomalib > v.0.4.0 Beta - Subject to Change
We introduce a new CLI approach that uses [PyTorch Lightning CLI](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html). To train a model using the new CLI, one would call the following:

```bash
anomalib fit --config <path/to/new/config/file>
```

For instance, to train a [PatchCore](https://github.com/openvinotoolkit/anomalib/tree/main/anomalib/models/patchcore) model, the following command would be run:

```bash
anomalib fit --config ./configs/model/patchcore.yaml
```

The new CLI approach offers a lot more flexibility, details of which are explained in the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html).

# Inference

## ⚠️ Anomalib < v.0.4.0

**推理速度与保存图片的像素有关**

The following command can be used to run inference from the command line:

```bash
python tools/inference/lightning_inference.py -h # new

python tools/inference.py \
    --config <path/to/model/config.yaml> \
    --weight_path <path/to/weight/file> \
    --image_path <path/to/image>
    --save_path <path/to/saveimage>
```

As a quick example:

```bash
python tools/inference/lightning_inference.py \
    --config anomalib/models/padim/config.yaml \
    --weights results/padim/mvtec/bottle/weights/model.ckpt \
    --input datasets/MVTec/bottle/test/broken_large/000.png \
    --output results/padim/mvtec/bottle/images
```

> character-small

```shell
python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-small/patchcore-224-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/1.abnormal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-small/patchcore-224-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-small/patchcore-384-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/1.abnormal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-small/patchcore-384-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-small/patchcore-512-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/1.abnormal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-small/patchcore-512-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal/ \
    --save_path output
```

> character-big
>
> 0.1

```shell
python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/1.abnormal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-768-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/1.abnormal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-768-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-1024-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/1.abnormal/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-1024-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal/ \
    --save_path output
```



```shell
python tools/inference.py `
    --config anomalib/models/patchcore/custom_config.yaml `
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt `
    --image_path datasets/some/1.abnormal/OriginImage_20220526_113038_Cam1_2_crop.jpg `
    --save_path output

python tools/inference.py `
    --config anomalib/models/patchcore/custom_config.yaml `
    --weight_path results/character-big/patchcore-768-0.1-n9/some/weights/model.ckpt `
    --image_path datasets/some/1.abnormal/OriginImage_20220526_113038_Cam1_2_crop.jpg `
    --save_path output

python tools/inference.py `
    --config anomalib/models/patchcore/custom_config.yaml `
    --weight_path results/character-big/patchcore-1024-0.1-n9/some/weights/model.ckpt `
    --image_path datasets/some/1.abnormal/OriginImage_20220526_113038_Cam1_2_crop.jpg `
    --save_path output
```

> new
>
> `datasets/some/0.normal-modify`

```shell
python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal-modify/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-768-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal-modify/ \
    --save_path output

python tools/inference.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-1024-0.1-n9/some/weights/model.ckpt \
    --image_path datasets/some/0.normal-modify/ \
    --save_path output
```

Example OpenVINO Inference:

```bash
python tools/inference/openvino_inference.py \
    --config anomalib/models/padim/config.yaml \
    --weights results/padim/mvtec/bottle/openvino/openvino_model.bin \
    --meta_data results/padim/mvtec/bottle/openvino/meta_data.json \
    --input datasets/MVTec/bottle/test/broken_large/000.png \
    --output results/padim/mvtec/bottle/images
```

> Ensure that you provide path to `meta_data.json` if you want the normalization to be applied correctly.

You can also use Gradio Inference to interact with the trained models using a UI. Refer to our [guide](https://openvinotoolkit.github.io/anomalib/guides/inference.html#gradio-inference) for more details.

A quick example:

```bash
python tools/inference/gradio_inference.py \
        --config ./anomalib/models/padim/config.yaml \
        --weights ./results/padim/mvtec/bottle/weights/model.ckpt
```

## Exporting Model to ONNX or OpenVINO IR

It is possible to export your model to ONNX or OpenVINO IR

If you want to export your PyTorch model to an OpenVINO model, ensure that `export_mode` is set to `"openvino"` in the respective model `config.yaml`.

```yaml
optimization:
  export_mode: "openvino" # options: openvino, onnx
```

## Export

> 注意： 导出patchcore时要将`anomalib/models/patchcore/anomaly_map.py` 60行的`gaussian_blur2d`注释掉，训练时再打开，注释掉的部分在使用部分添加上去
>
> 导出torchscript时可以选择cuda模式导出，根据网上的说法使用libtorch时调用显卡必须使用cuda导出模型，不过测试显示使用cpu导出的torchscript模型也能使用cuda推理
>
> 模型导出会导出模型 `output.onnx / output.torchscript` 和对应的超参数`param.json` 到result文件夹
>
> 导出的模型可以使用  `tools/read_onnx.py` `tools/read_torchscript.py` 进行推理
>
> 使用C++推理
>
> https://github.com/NagatoYuki0943/anomalib-libtorch
>
> https://github.com/NagatoYuki0943/anomalib-libtorch-cmake

```shell
python tools/export.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt \
    --image_size_height 512 \
    --image_size_width 512

python tools/export.py \
    --config anomalib/models/patchcore/custom_config.yaml \
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt \
    --image_size_height 512 \
    --image_size_width 512 \
    --format torchscript \
    --cuda True

#------------------------------------------------------------------------------------------#

python tools/export.py `
    --config anomalib/models/patchcore/custom_config.yaml `
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt `
    --image_size_height 512 `
    --image_size_width 512

python tools/export.py `
    --config anomalib/models/patchcore/custom_config.yaml `
    --weight_path results/character-big/patchcore-512-0.1-n9/some/weights/model.ckpt `
    --image_size_height 512 `
    --image_size_width 512 `
    --format torchscript `
    --cuda True

python tools/export.py `
    --config anomalib/models/patchcore/custom_config.yaml `
    --weight_path results/character-big/patchcore-768-0.1-n9/some/weights/model.ckpt `
    --image_size_height 768 `
    --image_size_width 768 `
    --format torchscript `
    --cuda True

python tools/export.py `
    --config anomalib/models/patchcore/custom_config.yaml `
    --weight_path results/character-big/patchcore-1024-0.1-n9/some/weights/model.ckpt `
    --image_size_height 1024 `
    --image_size_width 1024 `
    --format torchscript `
    --cuda True
```
