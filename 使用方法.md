# Install

## special requirements

> tested

```python
pytorch==1.12。1	# https://pytorch.org/get-started/previous-versions/#v1120
onnx==1.10.1
onnxruntime-gpu==1.10   # Compatible with numpy version used by openvino-dev
openvino-dev==2022.1.0
```

## Local Install

It is highly recommended to use virtual environment when installing anomalib. For instance, with [anaconda](https://www.anaconda.com/products/individual), `anomalib` could be installed as,

```bash
conda create -n anomalib python=3.9

conda activate anomalib

# install pytorch https://pytorch.org/get-started/locally/

git clone https://github.com/openvinotoolkit/anomalib.git
cd anomalib
pip install -e .
```

## Visualizer

`./anomalib/post_processing/visualizer.py` 调整保存图片的大小

```python
# figure_size = (num_cols * 5, 5)
figure_size = (num_cols * 15, 15)   # 调整图片大小
```

`./anomalib/utils/callbacks/visualizer_callback.py` 调用的是上面的`post_processing.py`中的`Visualizer`

`VisualizerCallback`是`callbacks`，会放进`Trainer`中运行。



# Training

## ⚠️ Anomalib < v.0.4.0

By default [`python tools/train.py`](https://github.com/openvinotoolkit/anomalib/blob/main/tools/train.py)
runs [PADIM](https://arxiv.org/abs/2011.08785) model on `leather` category from the [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) [(CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/) dataset.

```bash
python tools/train.py    # Train PADIM on MVTec AD leather
```

Training a model on a specific dataset and category requires further configuration. Each model has its own configuration
file, [`config.yaml`](https://github.com/openvinotoolkit/anomalib/blob/main/configs/model/padim.yaml)
, which contains data, model and training configurable parameters. To train a specific model on a specific dataset and
category, the config file is to be provided:

```bash
python tools/train.py --config <path/to/model/config.yaml>
```

For example, to train [PADIM](anomalib/models/padim) you can use

```bash
python tools/train.py --config anomalib/models/padim/config.yaml
```

Alternatively, a model name could also be provided as an argument, where the scripts automatically finds the corresponding config file.

>  max_epochs=1

```bash
python tools/train.py --config anomalib/models/patchcore/config.yaml	# stuck in 72%
python tools/train.py --config anomalib/models/padim/config.yaml		# fast
python tools/train.py --config anomalib/models/dfm/configa.yaml
python tools/train.py --config anomalib/models/dfkde/config.yaml
```

>  max_epochs>1

```shell
python tools/train.py --config anomalib/models/cflow/config.yaml
python tools/train.py --config anomalib/models/ganomaly/config.yaml
python tools/train.py --config anomalib/models/stfpm/config.yaml
python tools/train.py --config anomalib/models/fastflow/config.yaml
python tools/train.py --config anomalib/models/reverse_distillation/config.yaml
```

where the currently available models are:

- [CFlow](anomalib/models/cflow)
- [DFM](anomalib/models/dfm)
- [DFKDE](anomalib/models/dfkde)
- [FastFlow](anomalib/models/fastflow)
- [PatchCore](anomalib/models/patchcore)
- [PADIM](anomalib/models/padim)
- [STFPM](anomalib/models/stfpm)
- [GANomaly](anomalib/models/ganomaly)

## Feature extraction & (pre-trained) backbones

The pre-trained backbones come from [PyTorch Image Models (timm)](https://github.com/rwightman/pytorch-image-models), which are wrapped by `FeatureExtractor`.

For more information, please check our documentation or the [section about feature extraction in "Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide"](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055#b83b:~:text=ready%20to%20train!-,Feature%20Extraction,-timm%20models%20also>).

Tips:

- Papers With Code has an interface to easily browse models available in timm: [https://paperswithcode.com/lib/timm](https://paperswithcode.com/lib/timm)

- You can also find them with the function `timm.list_models("resnet*", pretrained=True)`

The backbone can be set in the config file, two examples below.

Anomalib < v.0.4.0

```yaml
model:
  name: cflow
  backbone: wide_resnet50_2
  pre_trained: true
Anomalib > v.0.4.0 Beta - Subject to Change
```

Anomalib >= v.0.4.0

```yaml
model:
  class_path: anomalib.models.Cflow
  init_args:
    backbone: wide_resnet50_2
    pre_trained: true
```

## Custom Dataset

It is also possible to train on a custom folder dataset. To do so, `data` section in `config.yaml` is to be modified as follows:

```yaml
dataset:
  name: <name-of-the-dataset>
  format: folder
  path: <path/to/folder/dataset>
  normal_dir: normal # name of the folder containing normal images.
  abnormal_dir: abnormal # name of the folder containing abnormal images.
  normal_test_dir: null # name of the folder containing normal test images.
  task: segmentation # classification or segmentation
  mask: <path/to/mask/annotations> #optional
  extensions: null
  split_ratio: 0.2 # ratio of the normal images that will be used to create a test split
  image_size: 256
  train_batch_size: 32
  test_batch_size: 32
  num_workers: 8
  transform_config:
    train: null
    val: null
  create_validation_set: true
  tiling:
    apply: false
    tile_size: null
    stride: null
    remove_border_count: 0
    use_random_tiling: False
    random_tile_count: 16
```

> example
```yaml
dataset:
  name: some
  format: folder
  path: ./datasets/images
  normal_dir: normal        # name of the folder containing normal images.
  abnormal_dir: abnormal    # name of the folder containing abnormal images.
  normal_test_dir: null     # name of the folder containing normal test images.
  task: classification      # classification or segmentation
  mask: null                # optional
  extensions: null
  split_ratio: 0.2          # ratio of the normal images that will be used to create a test split
  image_size: 224
  train_batch_size: 1
  test_batch_size: 1
  num_workers: 0
  transform_config:
    train: null
    val: null
  create_validation_set: false
  tiling:
    apply: false
    tile_size: null
    stride: null
    remove_border_count: 0
    use_random_tiling: False
    random_tile_count: 16
```

> 文件夹实例
```python
├── datasets
│   └── images
│       ├── normal/   <- normal images
│       └── abnormal/ <- abnormal images        
```

----

## ⚠️ Anomalib > v.0.4.0 Beta - Subject to Change

We introduce a new CLI approach that uses [PyTorch Lightning CLI](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html). To train a model using the new CLI, one would call the following:
```bash
anomalib fit --config <path/to/new/config/file>
```

For instance, to train a [PatchCore](https://github.com/openvinotoolkit/anomalib/tree/development/anomalib/models/patchcore) model, the following command would be run:
```bash
anomalib fit --config ./configs/model/patchcore.yaml
```

The new CLI approach offers a lot more flexibility, details of which are explained in the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html).

# Inference

## Anomalib < v.0.4.0

Anomalib includes multiple tools, including Lightning, Gradio, and OpenVINO inferencers, for performing inference with a trained model.

###  PyTorch Lightning inference

> 配置文件中的任务类别会影响推理`task: segmentation # classification or segmentation`

The following command can be used to run PyTorch Lightning inference from the command line:

```bash
python tools/inference/lightning_inference.py -h

def get_args() -> Namespace:
    """Get command line arguments.

    Returns:
        Namespace: List of arguments.
    """
    parser = ArgumentParser()
    parser.add_argument("--config", type=Path, required=True, help="Path to a config file")
    parser.add_argument("--weights", type=Path, required=True, help="Path to model weights")
    parser.add_argument("--input", type=Path, required=True, help="Path to image(s) to infer.")
    parser.add_argument("--output", type=str, required=False, help="Path to save the output image(s).")
    parser.add_argument(
        "--visualization_mode",
        type=str,
        required=False,
        default="simple",
        help="Visualization mode.",
        choices=["full", "simple"],
    )
    parser.add_argument(
        "--show",
        action="store_true",
        required=False,
        help="Show the visualized predictions on the screen.",
    )

    args = parser.parse_args()
    return args
```

As a quick example:

```bash
python tools/inference/lightning_inference.py \
    --config anomalib/models/padim/config.yaml \
    --weights results/padim/mvtec/bottle/weights/model.ckpt \
    --input datasets/MVTec/bottle/test/broken_large/000.png \
    --output results/padim/mvtec/bottle/images \
    --visualization_mode full

# 自己的
python tools/inference/lightning_inference.py \
    --config anomalib/models/patchcore/config.yaml \
    --weights results/patchcore/mvtec/bottle/weights/model.ckpt \
    --input datasets/MVTec/bottle/test/broken_large \
    --output results/patchcore/mvtec/bottle/infer \
    --visualization_mode full

# win
python tools/inference/lightning_inference.py `
    --config anomalib/models/patchcore/config.yaml `
    --weights results/patchcore/mvtec/bottle/weights/model.ckpt `
    --input datasets/MVTec/bottle/test/broken_large `
    --output results/patchcore/mvtec/bottle/infer `
    --visualization_mode full
```

### Torch Inference

`--task` 决定使用分类还是分割

推理时警告 `Transform configs has not been provided. Images will be normalized using ImageNet statistics.` 是没有问题的

```bash
def get_args() -> Namespace:
    """Get command line arguments.

    Returns:
        Namespace: List of arguments.
    """
    parser = ArgumentParser()
    parser.add_argument("--config", type=Path, required=True, help="Path to a config file")
    parser.add_argument("--weights", type=Path, required=True, help="Path to model weights")
    parser.add_argument("--input", type=Path, required=True, help="Path to an image to infer.")
    parser.add_argument("--output", type=Path, required=False, help="Path to save the output image.")
    parser.add_argument(
        "--task",
        type=str,
        required=False,
        help="Task type.",
        default="classification",
        choices=["classification", "segmentation"],
    )
    parser.add_argument(
        "--visualization_mode",
        type=str,
        required=False,
        default="simple",
        help="Visualization mode.",
        choices=["full", "simple"],
    )
    parser.add_argument(
        "--show",
        action="store_true",
        required=False,
        help="Show the visualized predictions on the screen.",
    )

    args = parser.parse_args()

    return args
```

Example Torch Inference:

```bash
python tools/inference/torch_inference.py \
    --config anomalib/models/patchcore/config.yaml \
    --weights results/patchcore/mvtec/bottle/weights/model.ckpt \
    --input datasets/MVTec/bottle/test/broken_large \
    --output results/patchcore/mvtec/bottle/infer \
    --task classification \
    --visualization_mode full

# win
python tools/inference/torch_inference.py `
    --config anomalib/models/patchcore/config.yaml `
    --weights results/patchcore/mvtec/bottle/weights/model.ckpt `
    --input datasets/MVTec/bottle/test/broken_large `
    --output results/patchcore/mvtec/bottle/infer `
    --task classification `
    --visualization_mode full
```

### OpenVINO Inference:

```bash
def get_args() -> Namespace:
    """Get command line arguments.

    Returns:
        Namespace: List of arguments.
    """
    parser = ArgumentParser()
    parser.add_argument("--config", type=Path, required=True, help="Path to a config file")
    parser.add_argument("--weights", type=Path, required=True, help="Path to model weights")
    parser.add_argument("--meta_data", type=Path, required=True, help="Path to a JSON file containing the metadata.")
    parser.add_argument("--input", type=Path, required=True, help="Path to an image to infer.")
    parser.add_argument("--output", type=Path, required=False, help="Path to save the output image.")
    parser.add_argument(
        "--task",
        type=str,
        required=False,
        help="Task type.",
        default="classification",
        choices=["classification", "segmentation"],
    )
    parser.add_argument(
        "--visualization_mode",
        type=str,
        required=False,
        default="simple",
        help="Visualization mode.",
        choices=["full", "simple"],
    )
    parser.add_argument(
        "--show",
        action="store_true",
        required=False,
        help="Show the visualized predictions on the screen.",
    )

    args = parser.parse_args()

    return args
```

Example OpenVINO Inference:

```bash
python tools/inference/openvino_inference.py \
    --config anomalib/models/patchcore/config.yaml \
    --weights results/patchcore/mvtec/bottle/optimization/openvino/model.bin \
    --meta_data results/patchcore/mvtec/bottle/optimization/meta_data.json \
    --input datasets/MVTec/bottle/test/broken_large \
    --output results/patchcore/mvtec/bottle/infer \
    --task classification \
    --visualization_mode full

# win
python tools/inference/openvino_inference.py `
    --config anomalib/models/patchcore/config.yaml `
    --weights results/patchcore/mvtec/bottle/optimization/openvino/model.bin `
    --meta_data results/patchcore/mvtec/bottle/optimization/meta_data.json `
    --input datasets/MVTec/bottle/test/broken_large `
    --output results/patchcore/mvtec/bottle/infer `
    --task classification `
    --visualization_mode full
```

> Ensure that you provide path to `meta_data.json` if you want the normalization to be applied correctly.

You can also use Gradio Inference to interact with the trained models using a UI. Refer to our [guide](https://openvinotoolkit.github.io/anomalib/guides/inference.html#gradio-inference) for more details.

A quick example:

```bash
python tools/inference/gradio_inference.py \
        --config ./anomalib/models/padim/config.yaml \
        --weights ./results/padim/mvtec/bottle/weights/model.ckpt
```

## Exporting Model to ONNX or OpenVINO IR

It is possible to export your model to ONNX or Torchscript or OpenVINO IR

If you want to export your PyTorch model to an OpenVINO model, ensure that `export_mode` is set to `"openvino"` in the respective model `config.yaml`.

导出openvino会先导出onnx, torchscript再导出openvino

导出onnx会一同导出torchscirpt

```yaml
optimization:
  export_mode: "openvino" # options: openvino, onnx
```

# Export

> 模型默认导出 `model.onnx / model_cpu.torchscript / model_gpu.torchscript` ,可以选择导出openvino
>
> 对应的超参数`param.json` 也会导出，导出位置为原来ckpt权重同路径的`export`文件夹
>
> 导出的模型可以使用  `tools/read_onnx.py` `tools/read_torchscript.py` 进行推理
>
> 使用C++推理
>
> https://github.com/NagatoYuki0943/anomalib-libtorch
>
> https://github.com/NagatoYuki0943/anomalib-libtorch-cmake



> 导出位置示例

```python
parent
├── weights
│   └── model.ckpt
└── export              <- export here
    ├── model.onnx
    ├── model_gpu.torchscript
    ├── model_cpu.torchscript
    ├── openvino/...
    └── meta_data.json
```

> 命令

- `--config` 模型配置文件
- `--weights` 模型权重文件
- `--img_size` 推理图片大小， 如 `224`, `[224,224]`, `224,224`
- `--openvino` 是否导出openvino

> ex: 回导出onnx, torchscript, openvino

`python tools/export.py  --config anomalib/models/patchcore/config.yaml --weights results/patchcore/mvtec/bottle/weights/model.ckpt --infer_size=[224,224] --openvino`



```python
def get_args() -> Namespace:
    """Get command line arguments.

    Returns:
        Namespace: List of arguments.
    """
    parser = ArgumentParser()
    parser.add_argument("--config", type=Path, required=True, help="Path to a config file")
    parser.add_argument("--weights", type=Path, required=True, help="Path to model weights")
    parser.add_argument("--infer_size", type=str, default=[224, 224], required=False,
                        help="infer image size(height, width), like 224 or [224,224] or 224,224")
    parser.add_argument("--openvino", default=False, action="store_true", required=False, help="export openvino")
    args = parser.parse_args()

    args.img_size = eval(args.img_size)
    # 如果宽高为整数,转换为数组
    if isinstance(args.img_size, int):
        args.img_size = [args.img_size, args.img_size]
    # tuple转换为list
    if isinstance(args.img_size, tuple):
        args.img_size = list(args.img_size)

    return args
```

> 读取导出文件的脚本在 `tools/read/` 目录下 
